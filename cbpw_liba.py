import os
import webbrowser
import time
from datetime import datetime, timedelta

import pandas as pd
import numpy as np

from IPython.display import display, clear_output

import plotly.graph_objects as go
import plotly.express as px

import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt

import mpld3
import mplcursors

import ipywidgets as widgets
from ipywidgets import interactive, VBox, HBox, interact

from scipy.spatial.distance import cdist
from scipy import stats

from sklearn.decomposition import PCA
from sklearn.cross_decomposition import CCA
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.diagnostic import linear_reset
import statsmodels.stats.api as sms
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
import statsmodels.tsa.api as smt
from statsmodels.stats.diagnostic import (
    acorr_ljungbox,
    het_white,
    het_breuschpagan,
    het_goldfeldquandt,
    het_arch,
)

import pychow
import requests

import streamlit as st


# –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
def drop_missing_values(df, reference_columns):
    df_cleaned = df.dropna()
    df_cleaned = df_cleaned[reference_columns]

    return df_cleaned


def normalized_dict_by_minmax(dict_of_years: dict[int, pd.DataFrame]):
    neutral = (
        'object_name',
        'object_level',
        'year'
    )

    normilized_dict = {}
    for year in dict_of_years.keys():
        normilized_dict[year] = dict_of_years[year].copy()
        for category in normilized_dict[year].columns:
            if category in neutral:
                continue
            normilized_dict[year][category] = (normilized_dict[year][category] - normilized_dict[year][
                category].min()) / (normilized_dict[year][category].max() - normilized_dict[year][category].min())
    return normilized_dict


def normalized_dict_by_minmax_by_base_year(dict_of_years: dict[int, pd.DataFrame], base_year = 2010):
    min_values_by_features = {}
    max_values_by_features = {}
    neutral = (
        'object_name',
        'object_level',
        'year'
    )

    for category in dict_of_years[base_year].columns:
        if category in neutral:
            continue
        min_values_by_features[category] = dict_of_years[base_year][category].min()
        max_values_by_features[category] = dict_of_years[base_year][category].max()

    normilized_dict = {}
    for year in dict_of_years.keys():
        normilized_dict[year] = dict_of_years[year].copy()
        for category in normilized_dict[year].columns:
            if category in neutral:
                continue
            normilized_dict[year][category] = (normilized_dict[year][category] - min_values_by_features[category]) / (
                        max_values_by_features[category] - min_values_by_features[category])
    return normilized_dict


def calculate_index_with_weights(dict_of_dataframes, way_of_calculating = 1):
    '''
    –í—ã—á–∏—Å–ª—è–µ—Ç –∏–Ω–¥–µ–∫—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤–µ—Å–æ–≤.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    dict_of_dataframes (dict): –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á–∏ - –≥–æ–¥—ã, –∞ –∑–Ω–∞—á–µ–Ω–∏—è - DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏.
    way_of_calculating (int): –ú–µ—Ç–æ–¥ —Ä–∞—Å—á–µ—Ç–∞ –≤–µ—Å–æ–≤:
        1 - PCA (–∞–Ω–∞–ª–∏–∑ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç);
        2 - –û–±—Ä–∞—Ç–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è;
        3 - –†–∞–≤–Ω—ã–µ –≤–µ—Å–∞.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π DataFrame —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –ø–æ –≥–æ–¥–∞–º –∏ –º–∞—Å—Å–∏–≤ –≤–µ—Å–æ–≤.
    '''

    indexed_data = {}
    combined_data = pd.concat(dict_of_dataframes.values())
    numerical_columns = combined_data.drop(columns = ['object_name', 'object_level', 'year'])

    if way_of_calculating == 1:  # PCA
        pca = PCA(n_components = 1)
        pca.fit(numerical_columns)
        weights = np.abs(pca.components_[0])
        weights /= weights.sum()

    elif way_of_calculating == 2:  # –û–±—Ä–∞—Ç–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        variances = numerical_columns.var()
        weights = 1 / variances
        weights /= weights.sum()

    else:  # –†–∞–≤–Ω—ã–µ –≤–µ—Å–∞
        num_columns = numerical_columns.shape[1]
        weights = np.ones(num_columns) / num_columns

    for year, df in dict_of_dataframes.items():
        numerical_columns = df.drop(columns = ['object_name', 'object_level', 'year'])
        weighted_mean = (numerical_columns * weights).sum(axis = 1)
        df['index'] = weighted_mean
        indexed_data[year] = df[['object_name', 'index']]

    combined_index = pd.DataFrame()
    for year, df in indexed_data.items():
        df = df.set_index('object_name')['index']
        combined_index[year] = df

    return combined_index, weights


def combine_indices(df1, df2, alpha):
    if not df1.shape == df2.shape:
        raise ValueError("–î–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (—Ä–∞–∑–º–µ—Ä—ã –∏ –∏–Ω–¥–µ–∫—Å—ã).")
    if not (df1.index.equals(df2.index) and df1.columns.equals(df2.columns)):
        raise ValueError("–î–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏ —Å—Ç–æ–ª–±—Ü—ã.")

    if not (0 <= alpha <= 1):
        raise ValueError("Alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 1.")

    combined_df = alpha * df1 + (1 - alpha) * df2
    return combined_df


def plot_index_trends_multi(dfs_by_base, dfs_by_current, dfs_final, region, domen):
    if not (dfs_by_base.index.equals(dfs_by_current.index) and dfs_by_current.index.equals(dfs_final.index)):
        raise ValueError("–ò–Ω–¥–µ–∫—Å—ã —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å –≤–æ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.")

    region_name = region
    base_index = dfs_by_base.loc[region]
    current_index = dfs_by_current.loc[region]
    final_index = dfs_final.loc[region]
    plt.figure(figsize = (14, 8))
    plt.plot(final_index.index, final_index.values, marker = 'o', linestyle = '-', linewidth = 3,
             label = "Final Index", color = 'red', alpha = 0.9)  # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    plt.plot(base_index.index, base_index.values, marker = 'o', linestyle = '--', linewidth = 2,
             label = "Base Index", color = 'blue', alpha = 0.5)
    plt.plot(current_index.index, current_index.values, marker = 'o', linestyle = ':', linewidth = 2,
             label = "Current Index", color = 'green', alpha = 0.5)
    plt.title(f"{domen}. –î–∏–Ω–∞–º–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ –≥–æ–¥–∞–º –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞: {region_name}", fontsize = 16, fontweight = 'bold')
    plt.xlabel("–ì–æ–¥", fontsize = 14)
    plt.ylabel("–ò–Ω–¥–µ–∫—Å", fontsize = 14)
    plt.grid(True, linestyle = '--', alpha = 0.7)
    plt.legend(fontsize = 12)
    for i in range(len(final_index)):
        plt.annotate(f"{final_index.values[i]:.2f}",
                     (final_index.index[i], final_index.values[i]),
                     textcoords = "offset points",
                     xytext = (0, 10),
                     ha = 'center', fontsize = 10, color = 'red')
    plt.xticks(rotation = 45)
    plt.tight_layout()
    plt.gca().set_facecolor('#f9f9f9')

    plt.show()


def plot_index_trends_with_similar(dfs_by_base, dfs_by_current, dfs_final, region_name, domen, k = 3):
    if not (dfs_by_base.index.equals(dfs_by_current.index) and dfs_by_current.index.equals(dfs_final.index)):
        raise ValueError("–ò–Ω–¥–µ–∫—Å—ã —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å –≤–æ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.")
    if region_name not in dfs_final.index:
        raise ValueError(f"–†–µ–≥–∏–æ–Ω '{region_name}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.")
    target_vector = dfs_final.loc[region_name].values.reshape(1, -1)
    distances = cdist(target_vector, dfs_final.values, metric = 'euclidean').flatten()
    similar_indices = np.argsort(distances)[1:k + 1]
    similar_regions = dfs_final.index[similar_indices]
    plt.figure(figsize = (14, 8))
    plt.gca().set_facecolor('white')
    plt.grid(color = 'lightgrey', linestyle = '--', linewidth = 0.5, alpha = 0.7)
    plt.plot(dfs_final.columns, dfs_final.loc[region_name],
             marker = 'o', linestyle = '-', linewidth = 3, label = f"{region_name} (Target)",
             color = 'orange', alpha = 0.9, zorder = 5)

    colors = ['#FF6347', '#4682B4', '#32CD32', '#FFD700', '#8A2BE2']

    for i, similar_region in enumerate(similar_regions):
        plt.plot(dfs_final.columns, dfs_final.loc[similar_region],
                 marker = 's', linestyle = ':', linewidth = 2, label = similar_region,
                 color = colors[i % len(colors)], alpha = 0.8)

        max_value = dfs_final.loc[similar_region].max()
        min_value = dfs_final.loc[similar_region].min()
        max_index = dfs_final.loc[similar_region].idxmax()
        min_index = dfs_final.loc[similar_region].idxmin()

        plt.scatter(max_index, max_value, color = colors[i % len(colors)], s = 100, zorder = 10, edgecolor = 'black',
                    marker = 'o')
        plt.scatter(min_index, min_value, color = colors[i % len(colors)], s = 100, zorder = 10, edgecolor = 'black',
                    marker = '^')

    max_value_target = dfs_final.loc[region_name].max()
    min_value_target = dfs_final.loc[region_name].min()
    max_index_target = dfs_final.loc[region_name].idxmax()
    min_index_target = dfs_final.loc[region_name].idxmin()
    plt.scatter(max_index_target, max_value_target, color = 'orange', s = 100, zorder = 10, edgecolor = 'black',
                marker = 'o', label = '–ú–∞–∫—Å–∏–º—É–º')
    plt.scatter(min_index_target, min_value_target, color = 'orange', s = 100, zorder = 10, edgecolor = 'black',
                marker = '^', label = '–ú–∏–Ω–∏–º—É–º')
    plt.title(f"{domen}. –î–∏–Ω–∞–º–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞ '{region_name}' –∏ {k} –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤", fontsize = 18,
              fontweight = 'bold', color = '#333')
    plt.xlabel("–ì–æ–¥", fontsize = 14)
    plt.ylabel("–ò–Ω–¥–µ–∫—Å", fontsize = 14)

    plt.legend(fontsize = 12, loc = "best")
    plt.xticks(rotation = 45)

    plt.tick_params(axis = 'both', which = 'major', labelsize = 12)

    plt.tight_layout()
    plt.show()


def plot_top_successful_regions_dynamic(dfs_final, domen, top_n = 10, alpha = 0.5):
    """
    –°—Ç—Ä–æ–∏—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —Ç–æ–ø-N —Å–∞–º—ã—Ö —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤, —É—á–∏—Ç—ã–≤–∞—è —É—Ä–æ–≤–µ–Ω—å –∏–Ω–¥–µ–∫—Å–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥ –∏ –¥–∏–Ω–∞–º–∏–∫—É.
    –î–æ–±–∞–≤–ª–µ–Ω—ã –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

    Args:
        dfs_final (pd.DataFrame): –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–Ω–¥–µ–∫—Å—ã - —Ä–µ–≥–∏–æ–Ω—ã, –∫–æ–ª–æ–Ω–∫–∏ - –≥–æ–¥—ã).
        top_n (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–º—ã—Ö —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        alpha (float): –í–µ—Å –¥–ª—è –¥–∏–Ω–∞–º–∏–∫–∏ (0 <= alpha <= 1).
                       –í–µ—Å —É—Ä–æ–≤–Ω—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≥–æ–¥–∞ = 1 - alpha.
    """
    if not (0 <= alpha <= 1):
        raise ValueError("–ü–∞—Ä–∞–º–µ—Ç—Ä alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1.")

    index_growth = dfs_final.diff(axis = 1).mean(axis = 1)
    last_year = dfs_final.columns[-1]
    last_year_index = dfs_final[last_year]
    combined_score = alpha * index_growth + (1 - alpha) * last_year_index
    top_regions = combined_score.sort_values(ascending = False).head(top_n)
    norm = plt.Normalize(min(top_regions), max(top_regions))
    cmap = matplotlib.colormaps['coolwarm']

    fig, ax = plt.subplots(figsize = (12, 7))  # –°–æ–∑–¥–∞–µ–º –æ—Å—å ax
    bars = ax.barh(top_regions.sort_values().index, top_regions.sort_values(),
                   color = cmap(norm(top_regions.sort_values())))

    for bar in bars:
        bar.set_edgecolor('black')
        bar.set_linewidth(1.5)
        for i, (region, value) in enumerate(zip(top_regions.sort_values().index, top_regions.sort_values())):
            ax.text(value + 0.01, i, f"({value:.3f})", va = 'center', fontsize = 12, color = 'black', weight = 'bold')

    ax.set_title(f"{domen}. –¢–æ–ø-{top_n} —Å–∞–º—ã—Ö —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å —É—á–µ—Ç–æ–º –¥–∏–Ω–∞–º–∏–∫–∏ ({last_year})", fontsize = 16,
                 weight = 'bold')
    ax.set_xlabel("–ö—É–º–º—É–ª—è—Ç–∏–≤–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥", fontsize = 14)
    ax.set_ylabel("–†–µ–≥–∏–æ–Ω—ã", fontsize = 14)
    ax.tick_params(axis = 'x', labelsize = 12)
    ax.tick_params(axis = 'y', labelsize = 12)

    sm = plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])

    ax.grid(axis = 'x', linestyle = '--', alpha = 0.7)
    plt.tight_layout()
    plt.show()


def plot_bottom_successful_regions_dynamic(dfs_final, domen, bottom_n = 10, alpha = 0.5):
    """
    –°—Ç—Ä–æ–∏—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É bottom-N –Ω–∞–∏–º–µ–Ω–µ–µ —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤, —É—á–∏—Ç—ã–≤–∞—è —É—Ä–æ–≤–µ–Ω—å –∏–Ω–¥–µ–∫—Å–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥ –∏ –¥–∏–Ω–∞–º–∏–∫—É.
    –î–æ–±–∞–≤–ª–µ–Ω—ã –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

    Args:
        dfs_final (pd.DataFrame): –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–Ω–¥–µ–∫—Å—ã - —Ä–µ–≥–∏–æ–Ω—ã, –∫–æ–ª–æ–Ω–∫–∏ - –≥–æ–¥—ã).
        bottom_n (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–∏–º–µ–Ω–µ–µ —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        alpha (float): –í–µ—Å –¥–ª—è –¥–∏–Ω–∞–º–∏–∫–∏ (0 <= alpha <= 1).
                       –í–µ—Å —É—Ä–æ–≤–Ω—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≥–æ–¥–∞ = 1 - alpha.
    """
    if not (0 <= alpha <= 1):
        raise ValueError("–ü–∞—Ä–∞–º–µ—Ç—Ä alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1.")

    index_growth = dfs_final.diff(axis = 1).mean(axis = 1)
    last_year = dfs_final.columns[-1]
    last_year_index = dfs_final[last_year]
    combined_score = alpha * index_growth + (1 - alpha) * last_year_index

    bottom_regions = combined_score.sort_values(ascending = True).head(bottom_n)
    norm = plt.Normalize(min(bottom_regions), max(bottom_regions))
    cmap = matplotlib.colormaps['Reds']  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–≤–µ—Ç–æ–≤—É—é —Å—Ö–µ–º—É –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    fig, ax = plt.subplots(figsize = (12, 7))
    bars = ax.barh(bottom_regions.sort_values(ascending = False).index,
                   bottom_regions.sort_values(ascending = False),
                   color = cmap(norm(bottom_regions.sort_values(ascending = False))))

    for bar in bars:
        bar.set_edgecolor('black')
        bar.set_linewidth(1.5)
        for i, (region, value) in enumerate(zip(bottom_regions.sort_values(ascending = False).index,
                                                bottom_regions.sort_values(ascending = False))):
            ax.text(value + 0.01, i, f"({value:.3f})", va = 'center', fontsize = 12, color = 'black', weight = 'bold')

    ax.set_title(f"{domen}. –¢–æ–ø-{bottom_n} –Ω–∞–∏–º–µ–Ω–µ–µ —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å —É—á–µ—Ç–æ–º –¥–∏–Ω–∞–º–∏–∫–∏ ({last_year})",
                 fontsize = 16, weight = 'bold', color = 'darkred')
    ax.set_xlabel("–ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥", fontsize = 14)
    ax.set_ylabel("–†–µ–≥–∏–æ–Ω—ã", fontsize = 14)
    ax.tick_params(axis = 'x', labelsize = 12)
    ax.tick_params(axis = 'y', labelsize = 12)

    sm = plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])

    ax.grid(axis = 'x', linestyle = '--', alpha = 0.7)
    plt.tight_layout()
    plt.show()


def mean_weight(weights_by_base, weights_by_current, alpha = 0.9):
    return weights_by_base * alpha + (1 - alpha) * weights_by_current


def count_impact_per_component(df: pd.DataFrame, weights: np.array):
    '''
    –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞:
        df : —Ç–µ–∫—É—â–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        weights : –≤–µ—Å–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    Return:
        –£–º–Ω–æ–∂–∞–µ–º –≤–µ—Å–∞ –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    '''
    buff_df = df.copy()
    buff_df.iloc[:, 3:-1] = buff_df.iloc[:, 3:-1] * weights
    return buff_df


def combine_for_weights_and_importances(dfs1, dfs2, index_share):
    '''
    –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
    –ù—É–∂–Ω–∞, —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª—è—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –¥–æ–ª—è—Ö
    '''
    dfs_final = {}
    for year in dfs1.keys():
        dfs_final[year] = dfs1[year].copy()
        dfs_final[year].iloc[:, 3:-1] = dfs_final[year].iloc[:, 3:-1] * index_share + dfs2[year].iloc[:, 3:-1] * (
                    1 - index_share)
        total_sum = dfs_final[year].iloc[:, 3:-1].sum(axis = 1)
        dfs_final[year] = dfs_final[year].drop('index', axis = 1)
        for col in dfs_final[year].columns[3:]:
            dfs_final[year][col] /= total_sum
    return dfs_final


def rank_importances(clear_domen, domen, importances_of_domen, region, year):
    if isinstance(year, list):
        start = year[0]
        end = year[1]
        df = importances_of_domen[start][importances_of_domen[start]['object_name'] == region].iloc[:, 3:].transpose()
        df.columns = ['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†']
        for year in range(start + 1, end + 1, 1):
            chosen = importances_of_domen[year][importances_of_domen[year]['object_name'] == region].iloc[:,
                     3:].transpose()
            chosen.columns = ['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†']
            df += chosen
        df['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†'] = df['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†'] / (end - start + 1)
        df = df.sort_values(by = '–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†', ascending = False)
        return df
    else:
        chosen = importances_of_domen[year][importances_of_domen[year]['object_name'] == region].iloc[:, 3:].transpose()
        chosen.columns = ['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†']
        values = clear_domen[year][clear_domen[year]['object_name'] == region].iloc[:, 3:].transpose()
        values.columns = ['–ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è']
        chosen = pd.concat([chosen, values], axis = 1)
        chosen = chosen.sort_values(by = '–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†', ascending = False)

        category = {'–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ': [], '–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ': []}
        clear = clear_domen[year][clear_domen[year]['object_name'] == region].iloc[:, 3:]
        modified = domen[year][domen[year]['object_name'] == region].iloc[:, 3:]

        for col in clear.columns:
            if clear[col].values == modified[col].values:
                category['–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ'].append(col)
            else:
                category['–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ'].append(col)
        chosen['–¢–∏–ø –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è'] = chosen.index.to_series().apply(
            lambda x: '–°—Ç–∏–º—É–ª—è–Ω—Ç' if x in category['–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ'] else '–î–µ—Å—Ç–∏–º—É–ª—è–Ω—Ç'
        )

        # –≥–∞—Ä–∞–Ω—Ç–∏—è –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏
        total = chosen['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†'].sum()
        chosen['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†'] = np.abs(chosen['–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†']) / total
        chosen = chosen.sort_values(by = '–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ò–†–†', ascending = False)

        return chosen


def ravel_domen_final(data):
    '''
    data: pd.DataFrame
    –í—ã—Ç—è–≥–∏–µ–≤–∞–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç–æ–ª–±—Ü–æ–≤: –≥–æ–¥, —Ä–µ–≥–∏–æ–Ω, –∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    '''
    data = data.reset_index()
    melted_data = data.melt(id_vars = [data.columns[0]],
                            var_name = '–≥–æ–¥',
                            value_name = '–∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞')

    melted_data.rename(columns = {data.columns[0]: '—Ä–µ–≥–∏–æ–Ω'}, inplace = True)

    return melted_data


def ravel_domen_dict(data_dict):
    '''
    data_dict: dict
    –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á–∏ - –≥–æ–¥—ã, –∑–Ω–∞—á–µ–Ω–∏—è - DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: object_name, object_level, year –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: –≥–æ–¥, object_name, object_level –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    '''
    df_list = []

    for year, df in data_dict.items():
        if df.empty:
            raise ValueError(f"DataFrame for year {year} is empty.")
        if not all(col in df.columns for col in ['object_name', 'object_level', 'year']):
            raise ValueError(f"DataFrame for year {year} is missing required columns.")

        df['year'] = year

        df_list.append(df)

    combined_df = pd.concat(df_list, ignore_index = True)

    return combined_df


def analyze_indicator(df, agg_func = "mean", region = None):
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –ø–æ –≥–æ–¥–∞–º, —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω—É.

    :param df: Dict[int : pd.DataFrame]
    :param agg_func: "mean" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –∏–ª–∏ "median" - –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    :param region: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ (–∏–ª–∏ None, —á—Ç–æ–±—ã —Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ –≤—Å–µ–º)
    """
    # 1Ô∏è‚É£ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω—É, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    df = ravel_domen_dict(df)
    if region:
        df = df[df["object_name"] == region]

    # 2Ô∏è‚É£ –í—ã–±–æ—Ä –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è
    indicators = df.columns[3:]  # –í—Å–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ—Å–ª–µ 'year'
    print("\n–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
    for i, col in enumerate(indicators, 1):
        print(f"{i}) {col}")

    choice = int(input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è: ")) - 1
    indicator = indicators[choice]

    # 3Ô∏è‚É£ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–ø–æ –≥–æ–¥–∞–º)
    if agg_func == "mean":
        df_grouped = df.groupby("year")[indicator].mean()
    elif agg_func == "median":
        df_grouped = df.groupby("year")[indicator].median()
    else:
        raise ValueError("–ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ agg_func. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'mean' –∏–ª–∏ 'median'.")

    # 4Ô∏è‚É£ –†–∞—Å—á–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –¥–ª—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    df_std = df.groupby("year")[indicator].std()

    # 5Ô∏è‚É£ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç—Ä–µ–Ω–¥ (–ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
    years = df_grouped.index.values
    y_values = df_grouped.values

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    slope, intercept, r_value, p_value, std_err = stats.linregress(years, y_values)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –Ω–∞–∫–ª–æ–Ω–∞
    if p_value < 0.05:  # –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ 0.05
        trend_slope = slope
        if trend_slope > 0:
            trend = "–í–æ—Å—Ö–æ–¥—è—â–∏–π üìà"
        elif trend_slope < 0:
            trend = "–ù–∏—Å—Ö–æ–¥—è—â–∏–π üìâ"
    else:
        trend = "–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–π ‚ûñ"

    region_str = f" ({region})" if region else " (–ø–æ –≤—Å–µ–º —Ä–µ–≥–∏–æ–Ω–∞–º)"
    print(f"\nüìä –¢—Ä–µ–Ω–¥ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è '{indicator}'{region_str}: {trend}")

    # 6Ô∏è‚É£ –¢–µ–º–ø—ã —Ä–æ—Å—Ç–∞ (% –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞)
    df_pct_change = df_grouped.pct_change() * 100

    # 7Ô∏è‚É£ –¢–æ–ø-3 –≥–æ–¥–∞ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é
    top_years = df_grouped.nlargest(3)
    print("\nüèÜ –¢–æ–ø-3 –≥–æ–¥–∞ –ø–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é:")
    print(top_years)

    # üîπ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è üîπ
    fig, axes = plt.subplots(2, 1, figsize = (12, 10))

    # --- –ü–µ—Ä–≤—ã–π –≥—Ä–∞—Ñ–∏–∫: –î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è ---
    axes[0].plot(df_grouped.index, df_grouped.values, marker = "o", label = "–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è", color = "b")
    axes[0].plot(years, intercept + slope * years, linestyle = "dashed", color = "red", label = "–õ–∏–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞")
    axes[0].fill_between(df_grouped.index, df_grouped - df_std, df_grouped + df_std, color = "gray", alpha = 0.2)

    axes[0].set_xlabel("–ì–æ–¥")
    # axes[0].set_title(f"–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è: {indicator}{region_str}")
    axes[0].legend()
    axes[0].grid(True)

    # --- –í—Ç–æ—Ä–æ–π –≥—Ä–∞—Ñ–∏–∫: –¢–µ–º–ø—ã —Ä–æ—Å—Ç–∞ (—Å—Ç–æ–ª–±–∏–∫–∞–º–∏) ---
    colors = ["green" if val >= 0 else "red" for val in df_pct_change.values]

    bars = axes[1].bar(df_pct_change.index, df_pct_change.values, color = colors, alpha = 0.7)

    # üî¢ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞–¥ —Å—Ç–æ–ª–±—Ü–∞–º–∏
    for bar in bars:
        height = bar.get_height()
        if not np.isnan(height):  # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ–±—ã –Ω–µ –ø–∏—Å–∞—Ç—å NaN
            axes[1].text(bar.get_x() + bar.get_width() / 2, height, f"{height:.1f}%",
                         ha = "center", va = "bottom" if height > 0 else "top", fontsize = 10, color = "black")

    axes[1].set_xlabel("–ì–æ–¥")
    axes[1].set_ylabel("–¢–µ–º–ø—ã –ø—Ä–∏—Ä–æ—Å—Ç–∞ (%)")
    axes[1].set_title(f"–¢–µ–º–ø—ã –ø—Ä–∏—Ä–æ—Å—Ç–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –ø–æ –≥–æ–¥–∞–º{region_str}")
    axes[1].axhline(0, color = "gray", linestyle = "dashed")  # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è –Ω–∞ 0%
    axes[1].grid(True)

    plt.tight_layout()


def analyze_indicator_interactive(df):
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ analyze_indicator —Å –≤—ã–±–æ—Ä–æ–º —Ä–µ–≥–∏–æ–Ω–∞ –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è.
    """
    df = ravel_domen_dict(df)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

    # –í–∏–¥–∂–µ—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞
    region_widget = widgets.Dropdown(
        options = [None] + list(df["object_name"].unique()),  # None = –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã
        value = None,
        description = "–†–µ–≥–∏–æ–Ω:"
    )

    # –í–∏–¥–∂–µ—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    agg_widget = widgets.RadioButtons(
        options = ["mean", "median"],
        value = "mean",
        description = "–ê–≥—Ä–µ–≥–∞—Ü–∏—è:"
    )

    # –í–∏–¥–∂–µ—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)
    indicator_widget = widgets.Dropdown(
        options = df.columns[3:],
        description = "–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å:"
    )

    # –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    def process(region, agg_func, indicator):
        df_filtered = df[df["object_name"] == region] if region else df

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        if agg_func == "mean":
            df_grouped = df_filtered.groupby("year")[indicator].mean()
        else:
            df_grouped = df_filtered.groupby("year")[indicator].median()

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
        df_std = df_filtered.groupby("year")[indicator].std()

        # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        years = df_grouped.index.values
        y_values = df_grouped.values
        slope, intercept, r_value, p_value, std_err = stats.linregress(years, y_values)

        trend = "–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–π ‚ûñ"
        if p_value < 0.05:
            if slope > 0:
                trend = "–í–æ—Å—Ö–æ–¥—è—â–∏–π üìà"
            elif slope < 0:
                trend = "–ù–∏—Å—Ö–æ–¥—è—â–∏–π üìâ"

        print(f"\nüìä –¢—Ä–µ–Ω–¥ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è '{indicator}' ({region if region else '–ø–æ –≤—Å–µ–º —Ä–µ–≥–∏–æ–Ω–∞–º'}): {trend}")

        # –¢–µ–º–ø—ã —Ä–æ—Å—Ç–∞
        df_pct_change = df_grouped.pct_change() * 100

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        fig, axes = plt.subplots(2, 1, figsize = (12, 10))

        # --- –ì—Ä–∞—Ñ–∏–∫ –¥–∏–Ω–∞–º–∏–∫–∏ ---
        axes[0].plot(df_grouped.index, df_grouped.values, marker = "o", label = "–î–∏–Ω–∞–º–∏–∫–∞", color = "b")
        axes[0].plot(years, intercept + slope * years, linestyle = "dashed", color = "red", label = "–¢—Ä–µ–Ω–¥")
        axes[0].fill_between(df_grouped.index, df_grouped - df_std, df_grouped + df_std, color = "gray", alpha = 0.2)
        axes[0].legend()
        axes[0].grid(True)

        # --- –ì—Ä–∞—Ñ–∏–∫ —Ç–µ–º–ø–æ–≤ —Ä–æ—Å—Ç–∞ ---
        colors = ["green" if val >= 0 else "red" for val in df_pct_change.values]
        bars = axes[1].bar(df_pct_change.index, df_pct_change.values, color = colors, alpha = 0.7)

        for bar in bars:
            height = bar.get_height()
            if not np.isnan(height):
                axes[1].text(bar.get_x() + bar.get_width() / 2, height, f"{height:.1f}%",
                             ha = "center", va = "bottom" if height > 0 else "top", fontsize = 10)

        axes[1].axhline(0, color = "gray", linestyle = "dashed")
        axes[1].grid(True)

        plt.tight_layout()
        plt.show()

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    interactive_plot = interactive(process,
                                   region = region_widget,
                                   agg_func = agg_widget,
                                   indicator = indicator_widget)

    display(interactive_plot)


def save_hdf5(data: dict[int, pd.DataFrame], filename: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å DataFrame –≤ HDF5 —Ñ–∞–π–ª.

    :param data: –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á–∏ - –≥–æ–¥—ã (int), –∞ –∑–Ω–∞—á–µ–Ω–∏—è - DataFrame.
    :param filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
    """
    with pd.HDFStore(filename, mode = "w") as store:
        for year, df in data.items():
            store.put(f"year_{year}", df)


def load_hdf5(filename: str) -> dict[int, pd.DataFrame]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ HDF5 —Ñ–∞–π–ª–∞ –≤ —Å–ª–æ–≤–∞—Ä—å DataFrame.

    :param filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
    :return: –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á–∏ - –≥–æ–¥—ã (int), –∞ –∑–Ω–∞—á–µ–Ω–∏—è - DataFrame.
    """
    data = {}
    with pd.HDFStore(filename, mode = "r") as store:
        for key in store.keys():
            year = int(key.split("_")[1])  # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥ –∏–∑ –∫–ª—é—á–∞
            data[year] = store[key]

    return data

def remove_highly_correlated_features(df: pd.DataFrame, correlation_threshold=0.9) -> pd.DataFrame:
    """
    –£–¥–∞–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
    
    :param df: pandas DataFrame, –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    :param correlation_threshold: float, –ø–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :return: pandas DataFrame, –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –±–µ–∑ –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    numeric_df = df.select_dtypes(include=[np.number])
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    corr_matrix = numeric_df.corr().abs()
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    
    # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]
    
    # –£–¥–∞–ª—è–µ–º –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    df_reduced = df.drop(columns=to_drop)
        
    return df_reduced

def process_dataframes_to_reduce_multicollinearity(dataframes: dict[int, pd.DataFrame], correlation_threshold=0.9) -> dict[int, pd.DataFrame]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤, —É–¥–∞–ª—è—è –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ.
    
    :param dataframes: Dict[int, pd.DataFrame], —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏
    :param correlation_threshold: float, –ø–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :return: Dict[int, pd.DataFrame], —Å–ª–æ–≤–∞—Ä—å —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏
    """
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –≤ –æ–¥–∏–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    combined_df = pd.concat(dataframes.values(), axis=0)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    numeric_combined_df = combined_df.select_dtypes(include=[np.number])
    corr_matrix = numeric_combined_df.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]
        
    # –£–¥–∞–ª—è–µ–º —ç—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    processed_dataframes = {key: df.drop(columns=to_drop) for key, df in dataframes.items()}
    
    return processed_dataframes

def to_one_structure(decomposed_SIIRD):
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞—Ä—å —Ç–∏–ø–∞ {std : pd.DataFrame} –∫ —Å–ª–æ–≤–∞—Ä—é —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ —à–µ–π–ø–∞–º–∏.
    –í—Å–µ DataFrame –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –∏ —Å—Ç–æ–ª–±—Ü–∞–º.

    Parameters:
    - decomposed_SIIRD (dict): –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á–∏ ‚Äî —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã, –∞ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî DataFrame.

    Returns:
    - dict: –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –≤—Å–µ DataFrame –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏ —Ñ–æ—Ä–º—ã, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –∏ —Å—Ç–æ–ª–±—Ü–∞–º.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã –∏ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –≤—Å–µ—Ö DataFrame
    common_index = None
    common_columns = None

    # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ —Å—Ç–æ–ª–±—Ü–æ–≤
    for key, df in decomposed_SIIRD.items():
        if common_index is None:
            common_index = df.index
        else:
            common_index = common_index.intersection(df.index)

        if common_columns is None:
            common_columns = df.columns
        else:
            common_columns = common_columns.intersection(df.columns)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏ —Å—Ç–æ–ª–±—Ü—ã
    common_index = sorted(common_index)
    common_columns = sorted(common_columns)

    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ DataFrame –∫ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∏–Ω–¥–µ–∫—Å–∞–º –∏ —Å—Ç–æ–ª–±—Ü–∞–º
    for key, df in decomposed_SIIRD.items():
        # –û–±—Ä–µ–∑–∞–µ–º DataFrame –ø–æ –æ–±—â–∏–º –∏–Ω–¥–µ–∫—Å–∞–º –∏ —Å—Ç–æ–ª–±—Ü–∞–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        decomposed_SIIRD[key] = df.loc[common_index, common_columns]
    
    return decomposed_SIIRD

def norm_final_domen_by_minmax_normalizing(data: pd.DataFrame) -> pd.DataFrame:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ DataFrame-–º–∞—Ç—Ä–∏—Ü—ã –ø–æ min-max –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏—Ö –≤ –æ–¥–∏–Ω.
    –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–¥–æ–ª—å –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤–¥–æ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–ø—Ä–∞–≤–¥–∞ —Ç–µ–º, —á—Ç–æ –º—ã —Ñ–∏–∫—Å–∏—Ä–∏—É–µ–º –∑–∞–¥–∞–Ω–Ω—ã–π –æ—Ç—Ä–µ–∑–æ–∫ –≤—Ä–µ–º–µ–Ω–∏ –∏ –∏–∑—É—á–∞–µ–º
    —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ —Ç–∞–∫–æ–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–µ. –¢–æ –µ—Å—Ç—å –Ω–∞–±–æ—Ä [data] –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫, –Ω–æ –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏. –û–Ω –ø–æ—Å—Ç—É–ª–∏—Ä—É–µ—Ç, —á—Ç–æ –≤ —Ä–∞–º–∫–∞—Ö
    –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –ª—É—á—à–µ–µ (—ç—Ç–æ 1) –∏ —Ö—É–¥—à–µ–µ (—ç—Ç–æ 0) –ø–æ–ª–æ–∂–µ–Ω–∏–µ –¥–µ–ª.

    :param data: DataFrame (–º–∞—Ç—Ä–∏—Ü–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ √ó –ª–µ—Ç)
    :return: –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
    """
    df = data.copy()
    x_max = df.max().max()
    x_min = df.min().min()
    return (df - x_min) / (x_max - x_min)


def compute_weights_for_SIIRD(decomposed_SIIRD, method='pca'):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –¥–ª—è –¥–æ–º–µ–Ω–æ–≤ –∏–∑ decomposed_SIIRD —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤.
    –ö–∞–∂–¥—ã–π DataFrame –∏–∑ decomposed_SIIRD –∏–º–µ–µ—Ç –∏–Ω–¥–µ–∫—Å—ã: —Ä–µ–≥–∏–æ–Ω—ã, –∫–æ–ª–æ–Ω–∫–∏: –≥–æ–¥–∞.
    
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞:
      1. –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ —Å—Ç–æ–ª–±—Ü—É (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ DataFrame —É–∂–µ –æ—á–∏—â–µ–Ω).
      2. ¬´–†–∞—Å—Ç—è–≥–∏–≤–∞–µ–º¬ª (flatten) –º–∞—Ç—Ä–∏—Ü—É –≤ –≤–µ–∫—Ç–æ—Ä.
    
    –ó–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º:
      - 'equal': —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞.
      - 'variance': –≤–µ—Å–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –¥–∏—Å–ø–µ—Ä—Å–∏–∏.
      - 'inverse_variance': –≤–µ—Å–∞, –æ–±—Ä–∞—Ç–Ω—ã–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏.
      - 'pca': –≤–µ—Å–∞, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–π –≥–ª–∞–≤–Ω–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      dict: —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á–∏ ‚Äì –∏–º–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–≤, –∞ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äì —Å–ª–æ–≤–∞—Ä–∏ –≤–µ—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞.
    """
    domains = list(decomposed_SIIRD.keys())
    vectors = {}
    for domain in domains:
        df = decomposed_SIIRD[domain].copy()
        # –ó–¥–µ—Å—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∏ —É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã (–µ—Å–ª–∏ –Ω–µ—Ç, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å fillna –∑–¥–µ—Å—å)
        vectors[domain] = df.values.flatten()
    
    # –ú–µ—Ç–æ–¥ 1: –†–∞–≤–Ω—ã–µ –≤–µ—Å–∞
    equal_weights = {domain: 1 / len(domains) for domain in domains}
    
    # –ú–µ—Ç–æ–¥ 2: –í–µ—Å–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –¥–∏—Å–ø–µ—Ä—Å–∏–∏
    var_dict = {domain: np.var(vectors[domain]) for domain in domains}
    total_var = sum(var_dict.values())
    variance_weights = {domain: var_dict[domain] / total_var if total_var != 0 else 1/len(domains)
                        for domain in domains}
    
    # –ú–µ—Ç–æ–¥ 3: –í–µ—Å–∞ –æ–±—Ä–∞—Ç–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏
    inv_var_dict = {domain: 1/np.var(vectors[domain]) if np.var(vectors[domain]) != 0 else 0
                    for domain in domains}
    total_inv = sum(inv_var_dict.values())
    inverse_variance_weights = {domain: inv_var_dict[domain] / total_inv if total_inv != 0 else 1/len(domains)
                                for domain in domains}
    
    # –ú–µ—Ç–æ–¥ 4: PCA
    X = np.column_stack([vectors[domain] for domain in domains])
    pca = PCA(n_components=1)
    pca.fit(X)
    loadings = np.abs(pca.components_[0])
    loadings = loadings / loadings.sum()
    pca_weights = {domain: loadings[i] for i, domain in enumerate(domains)}
    
    # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–π –º–µ—Ç–æ–¥
    if method == 'equal':
        return equal_weights
    elif method == 'variance':
        return variance_weights
    elif method == 'inverse_variance':
        return inverse_variance_weights
    elif method == 'pca':
        return pca_weights
    else:
        raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'equal', 'variance', 'inverse_variance' –∏–ª–∏ 'pca'.")

def compose_SIIRD(decomposed_SIIRD, weight_method='pca'):
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã–π —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å SIIRD.
    
    1. –ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Å–µ DataFrame –≤ decomposed_SIIRD –∫ –æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –∏ –≥–æ–¥—ã).
    2. –ó–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ —Å—Ç–æ–ª–±—Ü—É.
    3. –í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –º–µ—Ç–æ–¥—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é PCA).
    4. –°–∫–ª–∞–¥—ã–≤–∞–µ—Ç –≤—Å–µ –º–∞—Ç—Ä–∏—Ü—ã —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤.
    5. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É SIIRD —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ norm_final_domen_by_minmax_normalizing.
    
    Returns:
      pd.DataFrame: –ò—Ç–æ–≥–æ–≤—ã–π SIIRD.
    """
    # 1. –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    decomposed_SIIRD = to_one_structure(decomposed_SIIRD)
    
    # 2. –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ —Å—Ç–æ–ª–±—Ü—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
    for domain in decomposed_SIIRD:
        df = decomposed_SIIRD[domain]
        decomposed_SIIRD[domain] = df.apply(lambda col: col.fillna(col.mean()), axis=0)
    
    # 3. –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –¥–æ–º–µ–Ω–æ–≤ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –º–µ—Ç–æ–¥—É
    weights = compute_weights_for_SIIRD(decomposed_SIIRD, method=weight_method)
    
    # 4. –°–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Å–µ –º–∞—Ç—Ä–∏—Ü—ã, —É–º–Ω–æ–∂–∞—è –∫–∞–∂–¥—É—é –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–µ—Å
    SIIRD = None
    for domain, df in decomposed_SIIRD.items():
        weighted_df = df * weights[domain]
        if SIIRD is None:
            SIIRD = weighted_df.copy()
        else:
            SIIRD += weighted_df
    
    # 5. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É SIIRD —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ norm_final_domen_by_minmax_normalizing
    SIIRD_norm = norm_final_domen_by_minmax_normalizing(SIIRD)
    
    return SIIRD_norm

def compose_SFIIRD(SFIIRD, alpha=0.5):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∫–∞–∫ –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É FIIRD –∏ SIIRD.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      SFIIRD: dict, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–≤–∞ DataFrame:
             "FIIRD" ‚Äì —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å (—Ä–µ–≥–∏–æ–Ω—ã x –≥–æ–¥—ã)
             "SIIRD" ‚Äì —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å (—Ä–µ–≥–∏–æ–Ω—ã x –≥–æ–¥—ã)
      alpha: –≤–µ—Å –¥–ª—è FIIRD, –∞ (1 - alpha) ‚Äì –≤–µ—Å –¥–ª—è SIIRD.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      DataFrame —Å –∏—Ç–æ–≥–æ–≤—ã–º –∏–Ω–¥–µ–∫—Å–æ–º (—Ç–µ –∂–µ —Ä–µ–≥–∏–æ–Ω—ã –∏ –≥–æ–¥—ã).
    """
    FIIRD = SFIIRD["FIIRD"]
    SIIRD = SFIIRD["SIIRD"]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã –∏ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ)
    if not (FIIRD.index.equals(SIIRD.index) and FIIRD.columns.equals(SIIRD.columns)):
        raise ValueError("–ò–Ω–¥–µ–∫—Å—ã –∏/–∏–ª–∏ —Å—Ç–æ–ª–±—Ü—ã FIIRD –∏ SIIRD –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç.")
    
    final_index = alpha * FIIRD + (1 - alpha) * SIIRD
    return final_index

